{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Corpus.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPS0LWFd15be3vYIj7iYvdE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nevilshah235/Newspaper_Recommender_System/blob/master/Corpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klLmYZTG7fV6",
        "colab_type": "code",
        "outputId": "31b29679-d37b-4b95-807d-b191f8076f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import csv\n",
        "import pandas as pd\n",
        "import re,datetime\n",
        "start_time = time.time()\n",
        "\n",
        "class Corpus:\n",
        "    def __init__(self,web_url,start,end,filename,fieldnames):\n",
        "      self.web_url=web_url\n",
        "      self.start=start\n",
        "      self.end=end\n",
        "      self.rawdata=[]\n",
        "      self.article_id=[]\n",
        "      self.headline = []\n",
        "      self.dates = []\n",
        "      self.category = []\n",
        "      self.author = []\n",
        "      self.articles = []\n",
        "      self.filename=[]\n",
        "      self.fieldnames=fieldnames\n",
        "\n",
        "    def gen_corpus(self):\n",
        "      for i in range(self.start,self.end): #article numbers\n",
        "          self.article_id.append(str(int(i)))\n",
        "          rd=requests.get(self.web_url+'article/'+str(int(i))).text\n",
        "          self.rawdata.append(rd)\n",
        "      print(len(self.rawdata))\n",
        "      print(\"%s seconds\" % (time.time() - start_time))\n",
        "      return self.rawdata\n",
        "\n",
        "    def gen_corpus_attr(self):\n",
        "\n",
        "      for i in range (len(self.rawdata)):\n",
        "          # Turn page into BeautifulSoup object to access HTML tags\n",
        "          soup = bs(rawdata[i])\n",
        "\n",
        "          #to get headline\n",
        "          try:\n",
        "              head = soup.find('h1').get_text()\n",
        "          except:\n",
        "              head = None\n",
        "          #to get date  (the date in the webpeg was in the info class with some other info, so it needed to be extracted)\n",
        "          try:\n",
        "              date = soup.find_all(class_=\"article-info\")\n",
        "              datetext = [tag.get_text().strip() for tag in date]\n",
        "              date = datetext[-1]\n",
        "          except:\n",
        "              date = None\n",
        "          #to get category\n",
        "          try:\n",
        "              cat = soup.find(style=\"color:#777;\").get_text().strip()\n",
        "          except:\n",
        "              cat = None\n",
        "          #to get other\n",
        "          try:\n",
        "              auth = soup.find(class_=\"byline\").get_text()\n",
        "          except:\n",
        "              auth = None\n",
        "\n",
        "          #to extract text\n",
        "          p_tags = soup.find_all('p')\n",
        "          # Get the text from each of the \"p\" tags and strip surrounding whitespace.\n",
        "          p_tags_text = [tag.get_text().replace('\\xa0', ' ').strip('\\t') for tag in p_tags] #to remove non-breaking spaces from the document\n",
        "          article = ' '.join(p_tags_text)\n",
        "          self.headline.append(str(head))\n",
        "          self.dates.append(str(date))\n",
        "          self.category.append(str(cat))\n",
        "          self.author.append(str(auth))\n",
        "          self.articles.append(str(article))\n",
        "\n",
        "    def gen_corpusfile(self):\n",
        "      f = open(self.filename, \"w\")\n",
        "      writer = csv.DictWriter(f, fieldnames)\n",
        "      writer.writeheader()\n",
        "      f.close()\n",
        "      with open(self.filename, 'a') as f:\n",
        "          writer = csv.writer(f)\n",
        "          writer.writerows(zip(self.article_id, self.dates,self.category, self.headline,self.author, self.articles))\n",
        "\n",
        "c1=Corpus('https://scroll.in/',896000,896004,'scroll.csv',[\"Article_id\", \"Date\",\"category\",\"Headline\",\"Author\",\"Article content\"])\n",
        "c2=Corpus('https://historynewsnetwork.org/',170100,170105,'HNN.csv',[\"Article_id\", \"Date\",\"category\",\"Headline\",\"Author\",\"Article content\"])\n",
        "a=c1.gen_corpus()\n",
        "b=c2.gen_corpus()\n",
        "\n",
        "#896000 to 904500\n",
        "#170100 to 172101"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "7.7491984367370605 seconds\n",
            "5\n",
            "8.23460578918457 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVEN1GiW7zSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}